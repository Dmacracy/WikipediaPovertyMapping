{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17878011878058515067\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from scipy import stats\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model,load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, Input, Concatenate,concatenate, Reshape\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lx_median_2009-11_angola_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_burkina_faso_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_cameroon_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_ethiopia_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_lesotho_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_malawi_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_mozambique_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_nigeria_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_rwanda_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_senegal_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_tanzania_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_uganda_dhslocs_ee_export.tfrecord\r\n",
      "lx_median_2009-11_uganda_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2009-11_zimbabwe_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_benin_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_burkina_faso_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_cote_d_ivoire_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_drc_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_ghana_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_guinea_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_kenya_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_lesotho_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_malawi_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_mali_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_nigeria_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_rwanda_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_senegal_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_sierra_leone_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_togo_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_uganda_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2012-14_zambia_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_angola_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_ethiopia_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_ghana_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_kenya_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_malawi_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_mali_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_nigeria_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_tanzania_dhslocs_ee_export.tfrecord.gz\r\n",
      "lx_median_2015-17_zimbabwe_dhslocs_ee_export.tfrecord.gz\r\n"
     ]
    }
   ],
   "source": [
    "baseDir = \"/atlas/u/chenlin/wikipedia_images/Poverty_prediction/original_tfrecord/\"\n",
    "!ls \"/atlas/u/chenlin/wikipedia_images/Poverty_prediction/original_tfrecord/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Hyperparameters -----------\n",
    "\n",
    "band_names = ['BLUE', 'GREEN', \"RED\"]#, \"NIR\", \"SWIR1\", \"SWIR2\"]\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 10 \n",
    "TFRecord_files = [baseDir + 'lx_median_2015-17_nigeria_dhslocs_ee_export.tfrecord.gz']\n",
    "TFRecord_test = [baseDir + 'lx_median_2015-17_kenya_dhslocs_ee_export.tfrecord.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Load Doc2Vec ------------\n",
    "doc2vec_path = '/atlas/u/esheehan/wikipedia_project/dataset/GUF_dataset/data/AfricaArticleClustersUpdated.npy'\n",
    "doc2vecs = np.load(doc2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_cnn():\n",
    "    np.random.seed(1234)\n",
    "    model1 = Input(shape=(255, 255, 4))\n",
    "    x = Conv2D(3,(1,1))(model1)\n",
    "    base_model =  keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(255, 255, 3))\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "    x = base_model(x)\n",
    "    #base_model = load_model('/atlas/u/buzkent/Wikipedia_based_Learning/keras_model/wiki_global_full_FT.03.hdf5')\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(3010)(x)\n",
    "    model2 = Input(shape=(3010,))\n",
    "    merge = concatenate([x, model2])\n",
    "    merge = Dense(512, kernel_initializer='normal', activation='sigmoid')(merge)\n",
    "    merge = Dense(128, kernel_initializer='normal', activation='sigmoid')(merge)\n",
    "    merge = Dense(1, kernel_initializer='normal')(merge)\n",
    "    model = Model(inputs=[model1, model2], outputs=merge)\n",
    "    model.compile(loss='mse', optimizer='adam')#, metrics=[coeff_determination, 'mae'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_nightlight_summary():\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # For histogram\n",
    "    model1 = Input(shape=(3010,1))\n",
    "    x = Reshape((3010,), input_shape=(3010,1))(model1)\n",
    "    \n",
    "    # For summary stats\n",
    "#     model1 = Input(shape=(514,1))\n",
    "#     x = Reshape((514,), input_shape=(514,1))(model1)\n",
    "#     x = Dense(3010, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    model2 = Input(shape=(3010,))\n",
    "    merge = concatenate([x, model2])\n",
    "    merge = Dense(512, kernel_initializer='normal', activation='sigmoid')(merge)\n",
    "    merge = Dense(128, kernel_initializer='normal', activation='sigmoid')(merge)\n",
    "    merge = Dense(1, kernel_initializer='normal')(merge)\n",
    "    model = Model(inputs=[model1, model2], outputs=merge)\n",
    "    model.compile(loss='mse', optimizer='adam')#, metrics=[coeff_determination, 'mae'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses the TF record and returns everything\n",
    "def _parse_function(example_proto):\n",
    "    features = {\n",
    "        band_name : tf.FixedLenFeature(shape=[255**2], dtype=tf.float32) for band_name in ['LAT','LON','RED','GREEN','BLUE','NIGHTLIGHTS']\n",
    "    }\n",
    "    features['wealthpooled'] = tf.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    for band_name in band_names:\n",
    "        parsed_features[band_name] = tf.cast(parsed_features[band_name], tf.float64)\n",
    "    parsed_features['wealthpooled'] = tf.cast(parsed_features['wealthpooled'], tf.float64)\n",
    "    parsed_features['LAT'] = tf.cast(parsed_features['LAT'], tf.float64)\n",
    "    parsed_features['LON'] = tf.cast(parsed_features['LON'], tf.float64)\n",
    "    return parsed_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_embeding(doc2vecs, lat,lon):\n",
    "    closest = float('inf')\n",
    "    vec = None\n",
    "    for i in range(len(doc2vecs)):\n",
    "        if len(doc2vecs[i])>=5:\n",
    "            temp = (lat-doc2vecs[i][0][0])**2 + (lon-doc2vecs[i][0][1])**2\n",
    "            if temp<=closest:\n",
    "                closest = temp\n",
    "                vec = doc2vecs[i][5]\n",
    "    if vec:\n",
    "        return vec\n",
    "    return np.zeros((1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlot(x, y, label, xName, yName):\n",
    "    print(\"Plotting: \")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, '-b', label=label)\n",
    "    plt.xlabel(xName)\n",
    "    plt.ylabel(yName)\n",
    "    leg = ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualizeHistory(history):\n",
    "    for mode in [\"Train\", \"Val\"]:\n",
    "        for metric in [\"loss\", \"mse\", \"r2\", \"spearmanRho\", \"spearmanP\", \"pearsonRho\", \"pearsonP\"]:\n",
    "            if mode == \"Val\" and metric == \"loss\":\n",
    "                continue\n",
    "            cur = []\n",
    "            for epoch in range(EPOCH):\n",
    "                cur.append(history[(epoch, mode + \"_\" + metric)])\n",
    "                \n",
    "            maxVal = max(cur)\n",
    "\n",
    "            createPlot(range(EPOCH), cur, mode + \" \" + metric + \" MaxVal: \" \n",
    "                       +  str(maxVal), \"Epoch\", metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_embedding(doc2vecs, lat,lon):\n",
    "\n",
    "    closest = float('inf')\n",
    "    vec = None\n",
    "    for i in range(len(doc2vecs)):\n",
    "        if len(doc2vecs[i])>=5:\n",
    "            temp = (lat-doc2vecs[i][0][0])**2 + (lon-doc2vecs[i][0][1])**2\n",
    "            if temp<=closest:\n",
    "                closest = temp\n",
    "                vec = doc2vecs[i]\n",
    "    if vec is not None:\n",
    "        return concatenate_doc2vec(vec, 10), vec[2]\n",
    "    print(\"None Found\")\n",
    "    return np.zeros((1,300)), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_doc2vec(i, num):\n",
    "    embeddings = []\n",
    "    for index, article in enumerate(i[6]):\n",
    "        embeddings += list(article[4]) + [article[0]]\n",
    "        if index == num - 1:\n",
    "            break\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runOneEpoch(epoch, datasetPath, sess, history, mode=\"Train\"):\n",
    "    dataset = tf.data.TFRecordDataset(datasetPath, compression_type=\"GZIP\")\n",
    "    image = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "    image = image.batch(BATCH_SIZE)\n",
    "    iterator_image = image.make_one_shot_iterator()\n",
    "    next_image = iterator_image.get_next()\n",
    "    \n",
    "    all_labelsSave = []\n",
    "    all_predsSave = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_lat = []\n",
    "    all_lon = []\n",
    "        \n",
    "    count = 0\n",
    "    while True:\n",
    "#         print('Now training batch {}, epoch {}'.format(count, epoch))\n",
    "        Image = sess.run(next_image)\n",
    "        Index = Image['wealthpooled']\n",
    "\n",
    "# --------------- GET Images ---------------\n",
    "\n",
    "#         r = Image['RED']\n",
    "#         if r.shape[0] < BATCH_SIZE: \n",
    "#             break\n",
    "#         r = np.array(r).reshape(BATCH_SIZE,255, 255)\n",
    "#         g = Image['GREEN']\n",
    "#         g = np.array(g).reshape(BATCH_SIZE,255, 255)\n",
    "#         b = Image['BLUE']\n",
    "#         b = np.array(b).reshape(BATCH_SIZE,255, 255)\n",
    "#         n = Image['NIGHTLIGHTS']\n",
    "#         n = np.array(n.reshape(BATCH_SIZE,255, 255))\n",
    "#         stack_image = np.stack([r,g,b,n], axis=-1) \n",
    "\n",
    "        n = Image['NIGHTLIGHTS']\n",
    "        if n.shape[0] < BATCH_SIZE: \n",
    "            break\n",
    "        n = np.array(n.reshape(BATCH_SIZE,255, 255))\n",
    "    \n",
    "        # SUMMARY STATISTICS\n",
    "#         summaryN = []\n",
    "#         for curN in n: \n",
    "#             summaryN.append([np.max(curN), np.min(curN), np.mean(curN), \n",
    "#                              np.var(curN), *(stats.skew(curN)), *(stats.kurtosis(curN))])\n",
    "#         summaryN = np.asarray(summaryN)\n",
    "#         stack_image = np.stack([summaryN], axis=-1)\n",
    "#         print(np.max(curN))\n",
    "            \n",
    "        # HISTOGRAM STATISTICS\n",
    "        summaryN = []\n",
    "        for curN in n: \n",
    "            tmp = curN\n",
    "            tmp.flatten()\n",
    "            hist = np.histogram(tmp, bins=np.linspace(0,255,3010), density = True)\n",
    "            summaryN.append(np.asarray(hist[1]))\n",
    "\n",
    "        summaryN = np.asarray(summaryN)\n",
    "        stack_image = np.stack([summaryN], axis=-1)\n",
    "        \n",
    "        lat = np.mean(np.array(Image['LAT']),axis=1)\n",
    "        lon = np.mean(np.array(Image['LON']),axis=1)\n",
    "\n",
    "# --------------- GET DOC2Vec Embeddings --------------- \n",
    "        embedings = np.zeros((BATCH_SIZE,3010))\n",
    "        for i in range(len(lat)):\n",
    "            temp, index = find_embedding(doc2vecs, lat[i],lon[i])\n",
    "            embedings[i,:] = temp\n",
    "            if index is not None:\n",
    "                Index[i] = index\n",
    "\n",
    "# --------------- Metrics ---------------\n",
    "        if mode == \"Train\":\n",
    "            model_loss = model.train_on_batch([np.asarray(stack_image),embedings], np.concatenate(np.asarray(Index)))\n",
    "        \n",
    "        \n",
    "        prediction = model.predict([np.asarray(stack_image),embedings], batch_size=BATCH_SIZE)\n",
    "\n",
    "        all_labels.append(np.asarray(prediction))\n",
    "        all_preds.append(np.asarray(Index))\n",
    "            \n",
    "        count += 1\n",
    "\n",
    "        MSE = mean_squared_error(np.asarray(Index),np.asarray(prediction))\n",
    "        if mode == \"Train\":\n",
    "            history[(epoch, mode + '_loss')] += model_loss\n",
    "        history[(epoch, mode + '_mse')] += MSE\n",
    "    \n",
    "    \n",
    "# ------------- SAVING VALUES -------------\n",
    "        for i in range(len(lat)):\n",
    "            all_lat.append(lat[i])\n",
    "            all_lon.append(lon[i])\n",
    "            all_labelsSave.append(*prediction[i])\n",
    "            all_predsSave.append(*Index[i])\n",
    "        \n",
    "    \n",
    "# --------------- Summary Statistics ---------------\n",
    "    if mode == \"Train\":\n",
    "        history[(epoch, mode + '_loss')] /= count\n",
    "    history[(epoch, mode + '_mse')] /= count\n",
    "    \n",
    "    r2Score =  r2_score((np.concatenate(all_labels, axis=0)), (np.concatenate(all_preds, axis=0)))\n",
    "    (spearmanRho,spearmanP) = stats.spearmanr(np.concatenate(all_labels, axis=0), np.concatenate(all_preds, axis=0))\n",
    "    (pearsonRho, pearsonP) = pearsonr(np.concatenate(all_labels, axis=0),np.concatenate(all_preds, axis=0))\n",
    "    \n",
    "    history[(epoch, mode + '_r2')] = r2Score\n",
    "    history[(epoch, mode + \"_spearmanRho\")] = spearmanRho\n",
    "    history[(epoch, mode + \"_spearmanP\")] = spearmanP\n",
    "    history[(epoch, mode + \"_pearsonRho\")] = pearsonRho\n",
    "    history[(epoch, mode + \"_pearsonP\")] = pearsonP\n",
    "    \n",
    "    print(\"____________________\")\n",
    "    print(\"Epoch {}, average {} loss: {}, ave mse: {}, ave R2: {}\".format(epoch, mode,  history[(epoch, mode + '_loss')], \n",
    "                                                            history[(epoch, mode + '_mse')], history[(epoch, mode + '_r2')]))\n",
    "    print(\"Spearman: {},{}  Pearson: {}, {} \".format(spearmanRho, spearmanP, pearsonRho, pearsonP))\n",
    "    print(\"____________________\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# -------------- Return (lat,lon), predictions, results -------------\n",
    "    return all_lat, all_lon, all_labelsSave, all_predsSave\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tfrecord_paths, tfrecord_test, model, filename = \"\"):\n",
    "    \n",
    "    history = collections.defaultdict(int)\n",
    "    \n",
    "    print(\"Train path:\", tfrecord_paths)\n",
    "    print(\"Test Path: \", tfrecord_test)\n",
    "    image_size = 255\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(EPOCH):\n",
    "\n",
    "            print(\"Starting epoch: {}\".format(epoch))\n",
    "\n",
    "            _, _, _, _ = runOneEpoch(epoch, tfrecord_paths, sess, history, mode=\"Train\")\n",
    "            _, _, _, _ = runOneEpoch(epoch, tfrecord_test, sess, history, mode=\"Val\")\n",
    "    \n",
    "        print(\"Testing on filename: {}\".format(tfrecord_test))\n",
    "        all_lat, all_lon, all_labels, all_preds = runOneEpoch(epoch, tfrecord_test, sess, history, mode=\"Val\")\n",
    "        \n",
    "        if filename != \"\":\n",
    "            print(\"Saving to csv with filenames: {}, {}\".format(filename + \"_pred.csv\", filename + \"_true.csv\" ))\n",
    "            \n",
    "            all_lat = [\"lat\"] + all_lat\n",
    "            all_lon = [\"lon\"] + all_lon\n",
    "            all_labels = [\"wealthpooled\"] + all_labels\n",
    "            all_preds = [\"wealthpooled\"] + all_preds\n",
    "            \n",
    "            print(len(all_lat), len(all_lon), len(all_labels), len(all_preds))\n",
    "            print(all_lat[0], all_lat[1])\n",
    "            print(all_lon[0], all_lon[1])\n",
    "            print(all_labels[0], all_labels[1])\n",
    "            print(all_preds[0], all_preds[1])\n",
    "            \n",
    "#             np.savetxt(filename + '_pred.csv', [p for p in zip(all_lat, all_lon, \n",
    "#                                                 all_labels, all_preds)], delimiter=',', fmt='%s')\n",
    "            \n",
    "#             np.savetxt(filename + '_true.csv', [p for p in zip(all_lat, all_lon, \n",
    "#                                                 all_labels, all_preds)], delimiter=',', fmt='%s')\n",
    "\n",
    "#             print(\"Saving history with filename {}\".format(filename + \".npy\"))\n",
    "#             np.save(filename + \".npy\", history)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3010, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3010)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 3010)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6020)         0           reshape_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          3082752     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,148,545\n",
      "Trainable params: 3,148,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train path: ['/atlas/u/chenlin/wikipedia_images/Poverty_prediction/original_tfrecord/lx_median_2015-17_nigeria_dhslocs_ee_export.tfrecord.gz']\n",
      "Test Path:  ['/atlas/u/chenlin/wikipedia_images/Poverty_prediction/original_tfrecord/lx_median_2015-17_kenya_dhslocs_ee_export.tfrecord.gz']\n",
      "Starting epoch: 0\n",
      "____________________\n",
      "Epoch 0, average Train loss: 0.5324619427323342, ave mse: 0.49911048494654364, ave R2: -0.9474681347162877\n",
      "Spearman: 0.4552469918436684,8.907823750827725e-18  Pearson: [0.4422234], [9.39048989e-17] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 0, average Val loss: 0, ave mse: 0.6015377175961746, ave R2: -167080134675840.5\n",
      "Spearman: 0.10407838933373526,0.12037323410740171  Pearson: [0.00595548], [0.92937152] \n",
      "____________________\n",
      "Starting epoch: 1\n",
      "____________________\n",
      "Epoch 1, average Train loss: 1.0581791251897812, ave mse: 0.9767581787089767, ave R2: -5.270636103831818\n",
      "Spearman: -0.4021191637340699,7.24080568832183e-14  Pearson: [-0.38789142], [6.23119387e-13] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 1, average Val loss: 0, ave mse: 0.8691279163096367, ave R2: -245459611379220.75\n",
      "Spearman: 0.10407838933373526,0.12037323410740171  Pearson: [0.00300265], [0.96435587] \n",
      "____________________\n",
      "Starting epoch: 2\n",
      "____________________\n",
      "Epoch 2, average Train loss: 0.5932292491197586, ave mse: 0.5626552176165613, ave R2: -50.8742148903619\n",
      "Spearman: 0.3663616281152825,1.3361462482439187e-11  Pearson: [0.36473525], [1.66906849e-11] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 2, average Val loss: 0, ave mse: 0.5303155561559585, ave R2: -6.484674943088198e+16\n",
      "Spearman: 0.00438388523021797,0.9479787596167277  Pearson: [-0.02440017], [0.71645604] \n",
      "____________________\n",
      "Starting epoch: 3\n",
      "____________________\n",
      "Epoch 3, average Train loss: 0.6143602877855301, ave mse: 0.6172131275471285, ave R2: -442.8606762527237\n",
      "Spearman: -0.4063120763428605,3.764086841634212e-14  Pearson: [-0.41431459], [1.05242831e-14] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 3, average Val loss: 0, ave mse: 0.49771178894174023, ave R2: -8656806836144258.0\n",
      "Spearman: 0.09786475414963164,0.14428517352177966  Pearson: [0.02098441], [0.75477969] \n",
      "____________________\n",
      "Starting epoch: 4\n",
      "____________________\n",
      "Epoch 4, average Train loss: 0.6475105255842208, ave mse: 0.6313657960909882, ave R2: -79.80824376124652\n",
      "Spearman: -0.364194504744447,1.796712204400811e-11  Pearson: [-0.35080776], [1.06625487e-10] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 4, average Val loss: 0, ave mse: 0.6064497817712925, ave R2: -1.911844907842878e+16\n",
      "Spearman: 0.14311661599494865,0.03227216342173342  Pearson: [0.11994981], [0.07318508] \n",
      "____________________\n",
      "Starting epoch: 5\n",
      "____________________\n",
      "Epoch 5, average Train loss: 0.5929008394479751, ave mse: 0.5813101206980352, ave R2: -490.42306706700657\n",
      "Spearman: -0.12658827168337483,0.023528866078842987  Pearson: [-0.05457744], [0.33044603] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 5, average Val loss: 0, ave mse: 0.5454496614466181, ave R2: -6.878163297712987e+16\n",
      "Spearman: 0.14311661599494865,0.03227216342173342  Pearson: [0.11994981], [0.07318508] \n",
      "____________________\n",
      "Starting epoch: 6\n",
      "____________________\n",
      "Epoch 6, average Train loss: 0.6074129045009613, ave mse: 0.5997604850349874, ave R2: -329.01654846355825\n",
      "Spearman: -0.33658723745510655,6.463144899768661e-10  Pearson: [-0.3254475], [2.49036855e-09] \n",
      "____________________\n",
      "____________________\n",
      "Epoch 6, average Val loss: 0, ave mse: 0.5403795723391951, ave R2: -3.206696034706445e+16\n",
      "Spearman: -0.09786475414963164,0.14428517352177966  Pearson: [-0.03876023], [0.56388447] \n",
      "____________________\n",
      "Starting epoch: 7\n",
      "____________________\n",
      "Epoch 7, average Train loss: 0.6154312402009964, ave mse: 0.6042222318376345, ave R2: -217.29864378476088\n",
      "Spearman: -0.3182844672470712,5.762666930937422e-09  Pearson: [-0.31656931], [7.02192731e-09] \n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sailhome/esheehan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/numpy/lib/function_base.py:3183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/sailhome/esheehan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/numpy/lib/function_base.py:3184: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/sailhome/esheehan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/sailhome/esheehan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/sailhome/esheehan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "Epoch 7, average Val loss: 0, ave mse: 0.5633766620564824, ave R2: -1.0148891701227232e+16\n",
      "Spearman: nan,nan  Pearson: [-4.79886984e-17], [1.] \n",
      "____________________\n",
      "Starting epoch: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-58ad2d5226a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = load_model_cnn()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_nightlight_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFRecord_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFRecord_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrootPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-9536173cb7f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(tfrecord_paths, tfrecord_test, model, filename)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunOneEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfrecord_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunOneEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfrecord_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e987cd19cd7b>\u001b[0m in \u001b[0;36mrunOneEpoch\u001b[0;34m(epoch, datasetPath, sess, history, mode)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         print('Now training batch {}, epoch {}'.format(count, epoch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wealthpooled'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rootPath = \"/atlas/u/esheehan/wikipedia_project/dataset/GUF_dataset/output/doc2vecSat/\"\n",
    "filename = \"nigeria-zimbabwe.npy\"\n",
    "\n",
    "\n",
    "# model = load_model_cnn()\n",
    "model = load_model_nightlight_summary()\n",
    "history = train(TFRecord_files, TFRecord_test, model, filename= rootPath + filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
