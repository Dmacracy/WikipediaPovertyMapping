{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(DocModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        \n",
    "        self.docvecpipeline = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.hidden_size1, self.hidden_size2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, invecs, return_hidden=False):\n",
    "        hidden = self.docvecpipeline(invecs)\n",
    "        output = self.regressor(hidden)\n",
    "        if return_hidden:\n",
    "            return output, hidden\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocModelNet = DocModel(768, 512, 256)\n",
    "\n",
    "lr = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optim = optim.Adam(DocModelNet.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'rwanda'\n",
    "country_abrev = 'RWA'\n",
    "year = 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_wiki = pd.read_csv(f'articles/{country}_Wiki.csv')\n",
    "dhs_clusts = dhs_clusts = pd.read_csv('data/dhs_clusters.csv')\n",
    "country_clusts = country_clusts = dhs_clusts[(dhs_clusts['country']== country) & (dhs_clusts['year']== year)]\n",
    "# Convert string to np array because it was stored stupidly. Will fix later\n",
    "country_wiki['embedding'] = country_wiki['embedding'].apply(lambda x: np.fromstring(x[1:-1], \n",
    "                                                                                    sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(c1, c2):\n",
    "    '''\n",
    "    Compute approx distance between two coords given in (lat, long)\n",
    "    format. \n",
    "    '''\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "    lat1 = np.radians(c1[0])\n",
    "    lon1 = np.radians(c1[1])\n",
    "    lat2 = np.radians(c2[0])\n",
    "    lon2 = np.radians(c2[1])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def test_compute_dist():\n",
    "    '''\n",
    "    Unit test for compute_distance function.\n",
    "    \n",
    "    Uses Berkeley School of Infromation and Moscone Center South in San Fransisco\n",
    "    as a test case. Distance from Google maps. \n",
    "    '''\n",
    "    moscone_south = (37.783957939867015, -122.40107973374062) # lat, long\n",
    "    i_school = (37.871363468005065, -122.25852213941603) \n",
    "    correct_dist = 15.6 # in km\n",
    "    computed_dist = compute_distance(moscone_south, i_school)\n",
    "    np.testing.assert_approx_equal(computed_dist, correct_dist, significant=2)\n",
    "\n",
    "def get_dists_to_articles(country_clust, country_wiki):\n",
    "    '''\n",
    "    For a given DHS cluster, get the distance to each of the wiki \n",
    "    articles and return it as a numpy array.\n",
    "    '''\n",
    "    dists = []\n",
    "    for i in range(len(country_wiki)):\n",
    "        dist = compute_distance((country_clust['lat'], country_clust['lon']),\n",
    "                                (country_wiki['latitude'].iloc[i], country_wiki['longitude'].iloc[i]))\n",
    "        dists.append(dist)\n",
    "    return np.array(dists)\n",
    "\n",
    "def get_closest_n(dists_to_articles, n=10):\n",
    "    '''\n",
    "    Get the n closest articles to a given cluster.\n",
    "    Returns the indices ofthe articles and the approx distances in km.\n",
    "    '''\n",
    "    top_inds = np.argpartition(dists_to_articles, n)[:n]\n",
    "    return top_inds, dists_to_articles[top_inds]\n",
    "\n",
    "def get_input_tensor(country_clust,  country_wiki):\n",
    "    '''\n",
    "    For a given DHS cluster, get the input tensor that \n",
    "    we will feed to the model.\n",
    "    '''\n",
    "    embedds = []\n",
    "    for i, closest_idx in enumerate(country_clust['closest_article_idxs']):\n",
    "        embedds.append(country_wiki['embedding'].iloc[closest_idx])\n",
    "    dists = torch.tensor(country_clust['closest_article_dists']).float()\n",
    "    embedds = torch.flatten(torch.tensor(np.array(embedds))).float()\n",
    "    return embedds, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492/492 [00:03<00:00, 157.68it/s]\n",
      "<ipython-input-95-d97e46bf5222>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_clusts['dists_to_articles'] = country_clusts.progress_apply(lambda x : get_dists_to_articles(x, country_wiki), axis=1)\n"
     ]
    }
   ],
   "source": [
    "country_clusts['dists_to_articles'] = \\\n",
    "country_clusts.progress_apply(lambda x : get_dists_to_articles(x, country_wiki), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492/492 [00:00<00:00, 15842.26it/s]\n",
      "/home/dmac/.local/lib/python3.8/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "country_clusts[['closest_article_idxs', 'closest_article_dists']] = \\\n",
    "country_clusts.progress_apply(lambda x: get_closest_n(np.array(x['dists_to_articles'])), \n",
    "                              axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedds, dists = get_input_tensor(country_clusts.iloc[0], country_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Use the closest n to create input vectors for each cluster represented as \n",
    "# concatenations of the n closest article vectors and perhaps also the distance values\n",
    "# \n",
    "# Train model using those inputs and wealth scores as outputs. \n",
    "\n",
    "\n",
    "#get_closest_n(country_clusts['dists_to_articles'].iloc[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0725], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_model(torch.tensor(country_wiki['embedding'].iloc[0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        cond = torch.Tensor(infos[i]).to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu, cond.reshape((batch_size, ninfo))).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate batch of random conditional vectors\n",
    "        idx = probs.multinomial(1)\n",
    "        rand_cond = infos[idx]\n",
    "        rand_cond = rand_cond.reshape(rand_cond.shape[1:]).to(device)\n",
    "        #rand_title = titles[idx]\n",
    "        #rand_title = rand_title.reshape(rand_title.shape[1:]).to(device)\n",
    "        \n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise, rand_cond.reshape((batch_size, ninfo))) #, \n",
    "                   #rand_title.reshape((batch_size, ntitlevect)))\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach(), rand_cond.reshape((batch_size, ninfo))).view(-1)\n",
    "        #  rand_title.reshape((batch_size, ntitlevect))\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        \n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake, rand_cond.reshape((batch_size, ninfo))).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 50 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, fixed_cond.reshape((batch_size, ninfo))).detach().cpu() #,\n",
    "                           #fixed_titles.reshape((batch_size, ntitlevect))).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
