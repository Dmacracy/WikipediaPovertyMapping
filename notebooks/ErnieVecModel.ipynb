{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/.local/lib/python3.8/site-packages/tqdm/std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(DocModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        \n",
    "        modlist = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                modlist.append(nn.Sequential(\n",
    "                    nn.Linear(self.input_size, self.hidden_sizes[i]),\n",
    "                    nn.LeakyReLU()))\n",
    "            else:\n",
    "                modlist.append(nn.Sequential(\n",
    "                    nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]),\n",
    "                    nn.LeakyReLU()))\n",
    "                               \n",
    "        self.docvecpipeline = nn.ModuleList(modlist)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.hidden_sizes[-1], 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, invecs):\n",
    "        hidden = invecs\n",
    "        for i, layer in enumerate(self.docvecpipeline):\n",
    "            hidden = layer(hidden)\n",
    "        output = self.regressor(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_clusts = pd.read_csv('dhs_metadata_modified_wikis_np.csv')\n",
    "countries = list(dhs_clusts['country'].unique())\n",
    "wikis = {country : pd.read_csv(f'articles/{country}_Wiki.csv') for country in countries}\n",
    "for country, country_wiki in wikis.items():\n",
    "    country_wiki['embedding'] = country_wiki['embedding'].apply(lambda x: np.fromstring(x[1:-1], \n",
    "                                                                                        sep=' '))\n",
    "    country_wiki['w2vec'] = country_wiki['w2vec'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_folds = pickle.load(open(\"dhs_incountry_folds.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_embedds_from_idxs(row, wikis):\n",
    "    '''\n",
    "    Extract article features for a row of the DHS survey data, given some global\n",
    "    curr_country representing the current country and a corresponding article dataframe.\n",
    "    '''\n",
    "    country = row['country']\n",
    "    country_wiki = wikis[country]\n",
    "    embedds = []\n",
    "    w2vecs = []\n",
    "    for i, closest_idx in enumerate(row['article_closest_idxs']):\n",
    "        embedds.append(country_wiki['embedding'].iloc[int(closest_idx)])\n",
    "        w2vecs.append(country_wiki['w2vec'].iloc[int(closest_idx)])\n",
    "    embedds = torch.flatten(torch.tensor(np.array(embedds))).float().numpy()\n",
    "    w2vecs = torch.flatten(torch.tensor(np.array(w2vecs))).float().numpy()\n",
    "    \n",
    "    return embedds, w2vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_clusts['article_closest_idxs'] = dhs_clusts['article_closest_idxs'].apply(lambda x: np.fromstring(x[1:-1], \n",
    "                                                                                        sep=' '))\n",
    "dhs_clusts['article_dists'] = dhs_clusts['article_dists'].apply(lambda x: np.fromstring(x[1:-1], \n",
    "                                                                                        sep=' '))\n",
    "dhs_clusts['article_log2_lens'] = dhs_clusts['article_log2_lens'].apply(lambda x: np.fromstring(x[1:-1], \n",
    "                                                                                        sep=' '))\n",
    "dhs_clusts[['article_embeddings','w2vec']] = dhs_clusts.apply(lambda x: get_article_embedds_from_idxs(x, wikis), \n",
    "                                                              axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(dhs_clusts['article_embeddings'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 10222/19669 [04:33<04:14, 37.15it/s]"
     ]
    }
   ],
   "source": [
    "dhs_clusts['article_embeddings_proj'] = dhs_clusts['article_embeddings'].progress_apply(lambda x : pca.transform([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_inputs(row):\n",
    "    return torch.cat([torch.tensor([row['year']- 2009]),\n",
    "                      torch.tensor([row['lat']]),\n",
    "                      torch.tensor([row['lon']]), \n",
    "                      torch.tensor(row['article_dists']),\n",
    "                      torch.tensor(row['article_log2_lens']),\n",
    "                      torch.tensor([row['num_near_articles']]), \n",
    "                      torch.tensor(row['article_embeddings_proj']),\n",
    "                      torch.tensor(row['w2vec'])]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_clusts['tensor_input'] = dhs_clusts.apply(get_tensor_inputs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 768\n",
    "num_close_arts = 10\n",
    "\n",
    "DocModelNet = DocModel(dhs_clusts['tensor_input'].iloc[0].shape[0], \n",
    "                       [256*2, 256*2, 128*2, 64*2, 16*2])\n",
    "\n",
    "lr = 0.0001\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(DocModelNet.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DocModelNet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'A'\n",
    "train_idxs = dhs_folds[fold]['train']\n",
    "val_idxs = dhs_folds[fold]['val']\n",
    "test_idxs = dhs_folds[fold]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg loss per epoch\n",
    "train_epoch_losses = []\n",
    "val_epoch_losses = []\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # list of individ cluster losses for this epoch\n",
    "    train_iter_losses = []\n",
    "    val_iter_losses = []\n",
    "    # for each training point\n",
    "    for i in range(len(train_idxs)):\n",
    "        idx = train_idxs[i]\n",
    "        # clear gradient fro prev instance\n",
    "        DocModelNet.zero_grad()\n",
    "        # Get inputs\n",
    "        embedd = dhs_clusts['tensor_input'].iloc[idx].to(device)\n",
    "        # get target wealth value \n",
    "        target = torch.tensor(dhs_clusts['wealthpooled'].iloc[idx]).float().to(device)\n",
    "        # get model output\n",
    "        output = DocModelNet(embedd)\n",
    "        # Calculate loss based on this output and the loss funct\n",
    "        err = criterion(output[0], target)\n",
    "        # Calculate gradients\n",
    "        err.backward()\n",
    "        # Update network params using the optimizer\n",
    "        optimizer.step()\n",
    "        # Save Loss for plotting and analysis\n",
    "        train_iter_losses.append(err.item())\n",
    "    \n",
    "    # Evalaute model (but don't backpropagate!) on validation set\n",
    "    for i in range(len(val_idxs)):\n",
    "        idx = val_idxs[i]\n",
    "        # Get inputs\n",
    "        embedd = dhs_clusts['tensor_input'].iloc[idx].to(device)\n",
    "        # get target wealth value \n",
    "        target = torch.tensor(dhs_clusts['wealthpooled'].iloc[idx]).float().to(device)\n",
    "        # get model output\n",
    "        output = DocModelNet(embedd)\n",
    "        # Calculate loss based on this output and the loss funct\n",
    "        err = criterion(output[0], target)\n",
    "        # Save Loss for plotting and analysis\n",
    "        val_iter_losses.append(err.item())\n",
    "\n",
    "    train_epoch_losses.append(np.mean(train_iter_losses))\n",
    "    val_epoch_losses.append(np.mean(val_iter_losses))\n",
    "    print(f\"epoch {epoch}   avg train loss {train_epoch_losses[epoch]}   avg val loss {val_epoch_losses[epoch]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_wealth(clust, model, country_wiki):\n",
    "    embedd = clust['tensor_input'].to(device)\n",
    "    output = model(embedd)\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_clusts[fold + '_pred_wealth'] = \\\n",
    "dhs_clusts.apply(lambda clust : get_pred_wealth(clust, DocModelNet, country_wiki), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = Ridge(alpha=0)\n",
    "X = dhs_clusts.iloc[test_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)\n",
    "y = dhs_clusts.iloc[test_idxs]['wealthpooled'].values\n",
    "lin_model.fit(X, y)\n",
    "\n",
    "print(f'R^2 = {lin_model.score(X, y):.5f}')\n",
    "#r, p = ss.pearsonr(X[:,0], y)\n",
    "#print('pearson r^2: ', r**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_clusts['pred_wealth'].to_csv(f'early_{country}_({year})_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1,figsize=(10,30))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title(\"DHS Wealth Indices vs Predicted Train Set\")\n",
    "\n",
    "plt.plot(dhs_clusts.iloc[train_idxs][fold + '_pred_wealth'].values, \n",
    "         dhs_clusts.iloc[train_idxs]['wealthpooled'].values, 'bo', alpha=0.5);\n",
    "\n",
    "lin_model = Ridge(alpha=0)\n",
    "X = dhs_clusts.iloc[train_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)\n",
    "y = dhs_clusts.iloc[train_idxs]['wealthpooled'].values\n",
    "lin_model.fit(X, y)\n",
    "R = f\"{lin_model.score(X, y):.3f}\"\n",
    "plt.plot(dhs_clusts.iloc[train_idxs][fold + '_pred_wealth'].values,\n",
    "        lin_model.predict(dhs_clusts.iloc[train_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)),\n",
    "        'k-', alpha=0.9, label=r'Train $R^2=$'+R)\n",
    "plt.xlabel(\"Predicted Wealth Index\")\n",
    "plt.ylabel(\"Actual Wealth Index\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"DHS Wealth Indices vs Predicted Validation Set\")\n",
    "plt.plot(dhs_clusts.iloc[val_idxs][fold + '_pred_wealth'].values, \n",
    "         dhs_clusts.iloc[val_idxs]['wealthpooled'].values, 'bo', alpha=0.5);\n",
    "lin_model = Ridge(alpha=0)\n",
    "X = dhs_clusts.iloc[val_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)\n",
    "y = dhs_clusts.iloc[val_idxs]['wealthpooled'].values\n",
    "lin_model.fit(X, y)\n",
    "R = f\"{lin_model.score(X, y):.3f}\"\n",
    "plt.plot(dhs_clusts.iloc[val_idxs][fold + '_pred_wealth'].values,\n",
    "        lin_model.predict(dhs_clusts.iloc[val_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)),\n",
    "        'k-', alpha=0.9, label=r'Val $R^2=$'+R)\n",
    "plt.xlabel(\"Predicted Wealth Index\")\n",
    "plt.ylabel(\"Actual Wealth Index\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"DHS Wealth Indices vs Predicted Test Set\")\n",
    "plt.plot(dhs_clusts.iloc[test_idxs][fold + '_pred_wealth'].values, \n",
    "         dhs_clusts.iloc[test_idxs]['wealthpooled'].values, 'bo', alpha=0.5);\n",
    "lin_model = Ridge(alpha=0)\n",
    "X = dhs_clusts.iloc[test_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)\n",
    "y = dhs_clusts.iloc[test_idxs]['wealthpooled'].values\n",
    "lin_model.fit(X, y)\n",
    "R = f\"{lin_model.score(X, y):.3f}\"\n",
    "plt.plot(dhs_clusts.iloc[test_idxs][fold + '_pred_wealth'].values,\n",
    "        lin_model.predict(dhs_clusts.iloc[test_idxs][fold + '_pred_wealth'].values.reshape(-1, 1)),\n",
    "        'k-', alpha=0.9, label=r'Test $R^2=$'+R)\n",
    "plt.xlabel(\"Predicted Wealth Index\")\n",
    "plt.ylabel(\"Actual Wealth Index\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'{fold}_preds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dhs_clusts.drop(columns=['article_embeddings', 'tensor_input']).to_csv('dhs_metadata_modified_wikis_np.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally save the model\n",
    "\n",
    "# save_model = False\n",
    "# checkpoint_path = f\"./models/articles/{fold}\"\n",
    "# if save_model:\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': DocModelNet.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict()\n",
    "#             }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = dhs_clusts['wealthpooled'].quantile(q=np.arange(0.05, 1.05, 0.05)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = []\n",
    "for percentile in percentiles:\n",
    "    lin_model = Ridge(alpha=0)\n",
    "    X = dhs_clusts.iloc[test_idxs][dhs_clusts.iloc[test_idxs]['wealthpooled'] <= percentile][fold + '_pred_wealth'].values.reshape(-1, 1)\n",
    "    y = dhs_clusts.iloc[test_idxs][dhs_clusts.iloc[test_idxs]['wealthpooled'] <= percentile]['wealthpooled'].values\n",
    "    lin_model.fit(X, y)\n",
    "\n",
    "    #print(f'R^2 = {lin_model.score(X, y):.5f}')\n",
    "    rs.append(lin_model.score(X, y))\n",
    "    \n",
    "fig, axs = plt.subplots(1,1,figsize=(10,10))\n",
    "axs.plot(np.arange(0.05, 1.05, 0.05), rs, '-o');\n",
    "plt.xlabel(\"Percentile Wealth Index\")\n",
    "plt.ylabel(r\"Pearson $r^2$ of Actual and Predicted Wealth\")\n",
    "plt.title(\"Model Performance at Different Wealth Levels\");\n",
    "plt.savefig(f'{fold}_percentile_preds.png')\n",
    "#r, p = ss.pearsonr(X[:,0], y)\n",
    "#print('pearson r^2: ', r**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with other Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in second country data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country2 = 'malawi'\n",
    "country2_abrev = 'MWI'\n",
    "year_country2 = [2014, 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country2_clusts['pred_wealth'] = \\\n",
    "country2_clusts.apply(lambda clust : get_pred_wealth(clust, DocModelNet, country2_wiki), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = Ridge(alpha=0)\n",
    "X = country2_clusts['pred_wealth'].values.reshape(-1, 1)\n",
    "y = country2_clusts['wealthpooled'].values\n",
    "lin_model.fit(X, y)\n",
    "\n",
    "print('R^2 with no regularization (alpha=0): ', lin_model.score(X, y))\n",
    "\n",
    "fold_n = 10\n",
    "kf = KFold(n_splits=fold_n)\n",
    "Rs = []\n",
    "for train, test in kf.split(X, y):\n",
    "    clf = Ridge(alpha=0.5).fit(X[train], y[train])\n",
    "    Rs.append(clf.score(X[test], y[test]))\n",
    "Rs = np.array(Rs)\n",
    "print('Avg cross-validated R^2 with 10 folds and alpha = 0.5:', Rs.mean())\n",
    "print('Standard Deviation of R^2 measurements:', Rs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country2_clusts['pred_wealth'].to_csv(f'early_{country}_({year})_to_{country2}_({year_country2})_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "country2_cap = country2[0].upper() + country2[1:]\n",
    "plt.title(f\"{country2_cap} {year_country2} DHS Wealth Indices vs Predicted From Model Trained on {country_cap} {year} Data\")\n",
    "plt.plot(country2_clusts['pred_wealth'].values, \n",
    "         country2_clusts['wealthpooled'].values, 'bo', alpha=0.7);\n",
    "R = f\"{lin_model.score(X, y):.4f}\"\n",
    "plt.plot(country2_clusts['pred_wealth'].values,\n",
    "        lin_model.predict(country2_clusts['pred_wealth'].values.reshape(-1, 1)),\n",
    "        'k-', alpha=0.7, label=r'$R^2=$'+R)\n",
    "plt.xlabel(\"Predicted Wealth Index\")\n",
    "plt.ylabel(\"Actual Wealth Index\")\n",
    "plt.legend()\n",
    "plt.savefig(f'early_{country}_({year})_to_{country2}_({year_country2})_preds.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
